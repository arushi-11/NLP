{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "345_InformationExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF5pbFjxJA9S",
        "outputId": "82857089-1809-4742-cd4a-4015170cadf7"
      },
      "source": [
        "#Importing packgaes\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from google.colab import drive\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fac2yOKvJdmr"
      },
      "source": [
        "df_idf=pd.read_json(\"/content/drive/MyDrive/NLP- Assignments/stackoverflow-test.json\",lines=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-r8Jfw9JnKx"
      },
      "source": [
        "#Function to pre-process the documents, in order \r\n",
        "#to conert them into lower case, removal of tags special character and digits.\r\n",
        "def pre_process(text):\r\n",
        "    \r\n",
        "    # lowercase\r\n",
        "    text=text.lower()\r\n",
        "    \r\n",
        "    #remove tags\r\n",
        "    text=re.sub(\"</?.*?>\",\" <> \",text)\r\n",
        "    \r\n",
        "    # remove special characters and digits\r\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\r\n",
        "    \r\n",
        "    return text"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "3dDe2o5BJsVt",
        "outputId": "5ab47def-376f-4977-9f9c-422bdd394fb5"
      },
      "source": [
        "#Only title and body is needed, so new dataframe is created and pre-processed\r\n",
        "df_idf['text'] = df_idf['title'] + df_idf['body']\r\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\r\n",
        "df_idf['text'][1]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'phantomjs node page evaulate seems to hang i have an implementation of waitfor with phantomjs node and it seems that the sitepage evaluate has a big lag compared to when it should evaluate true you ll see below that i m logging out the content value and the content logs with what should evaluate as true but this doesn t seem to occur for a good seconds or so after the fact any idea what would cause this delay or if there s a better way to evaluate let promise require bluebird let phantom require phantom let sitepage let phinstance phantom create then instance gt phinstance instance return instance createpage then page gt sitepage page return page open https thepiratebay org search game then status gt return waituntil function this returns the correct content after a short period while the evaluate ends up taking maybe s longer after this content should evaluate true sitepage property content then content gt console log content return sitepage evaluate function return document getelementbyid searchresult then function return sitepage property content catch promise timeouterror function e sitepage close phinstance exit then content gt console log content console log content sitepage close phinstance exit catch error gt console log error phinstance exit var waituntil asynctest gt return new promise function resolve reject function wait console log waiting asynctest then function value if value console log resolve resolve else settimeout wait catch function e console log error found rejecting e reject wait '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNw60N3KJtZp",
        "outputId": "1cc3ad33-97bb-4aea-8517-c8f0bf29799d"
      },
      "source": [
        "def get_stop_words(stop_file_path):\r\n",
        "    \"\"\"load stop words \"\"\"\r\n",
        "    \r\n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\r\n",
        "        stopwords = f.readlines()\r\n",
        "        stop_set = set(m.strip() for m in stopwords)\r\n",
        "        return frozenset(stop_set)\r\n",
        "\r\n",
        "#load a set of stop words\r\n",
        "stopwords=get_stop_words(\"/content/drive/MyDrive/NLP- Assignments/stopwords.txt\")\r\n",
        "\r\n",
        "#get the text column \r\n",
        "docs=df_idf['text'].tolist()\r\n",
        "\r\n",
        "#create a vocabulary of words, \r\n",
        "#ignore words that appear in 85% of documents, \r\n",
        "#eliminate stop words\r\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords)\r\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "civZf5CYJ_wY",
        "outputId": "0f1876b4-19eb-4e39-f684-d161bddf813c"
      },
      "source": [
        "#Making the count vector with vocabulary size 10000\r\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=stopwords,max_features=10000)\r\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dbKDSBzKDgA",
        "outputId": "7d6b4581-89e0-491c-9688-f1553fe2a345"
      },
      "source": [
        "#Creation of tf-idf vector using created count vector\r\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\r\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4yH0VweKJRs"
      },
      "source": [
        "# read test docs into a dataframe and concatenate title and body\r\n",
        "df_test=pd.read_json(\"/content/drive/MyDrive/NLP- Assignments/stackoverflow-test.json\",lines=True)\r\n",
        "df_test['text'] = df_test['title'] + df_test['body']\r\n",
        "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\r\n",
        "\r\n",
        "# get test docs into a list\r\n",
        "docs_test=df_test['text'].tolist()\r\n",
        "docs_title=df_test['title'].tolist()\r\n",
        "docs_body=df_test['body'].tolist()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDDoeRR7KOt9"
      },
      "source": [
        "feature_names = cv.get_feature_names()\r\n",
        "def sort_coo(coo_matrix):\r\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\r\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\r\n",
        "\r\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\r\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\r\n",
        "    \r\n",
        "    #use only topn items from vector\r\n",
        "    sorted_items = sorted_items[:topn]\r\n",
        "\r\n",
        "    score_vals = []\r\n",
        "    feature_vals = []\r\n",
        "\r\n",
        "    for idx, score in sorted_items:\r\n",
        "        fname = feature_names[idx]\r\n",
        "        \r\n",
        "        #keep track of feature name and its corresponding score\r\n",
        "        score_vals.append(round(score, 3))\r\n",
        "        feature_vals.append(feature_names[idx])\r\n",
        "\r\n",
        "    #create a tuples of feature,score\r\n",
        "    #results = zip(feature_vals,score_vals)\r\n",
        "    results= {}\r\n",
        "    for idx in range(len(feature_vals)):\r\n",
        "        results[feature_vals[idx]]=score_vals[idx]\r\n",
        "    \r\n",
        "    return results"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07auycc3KVBT"
      },
      "source": [
        "def get_keywords(idx):\r\n",
        "\r\n",
        "    #generate tf-idf for the given document\r\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\r\n",
        "\r\n",
        "    #sort the tf-idf vectors by descending order of scores\r\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\r\n",
        "\r\n",
        "    #extract only the top n; n here is 10\r\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\r\n",
        "    \r\n",
        "    return keywords\r\n",
        "\r\n",
        "def print_results(idx,keywords):\r\n",
        "    # now print the results\r\n",
        "    print(\"\\n=====Title=====\")\r\n",
        "    print(docs_title[idx])\r\n",
        "    print(\"\\n=====Body=====\")\r\n",
        "    print(docs_body[idx])\r\n",
        "    print(\"\\n===Keywords===\")\r\n",
        "    for k in keywords:\r\n",
        "        print(k,keywords[k])\r\n",
        "\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-1O3GCpKZV6",
        "outputId": "d40c1b2f-48e1-4732-868a-a13f442e778a"
      },
      "source": [
        "idx=120\r\n",
        "keywords=get_keywords(idx)\r\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "SQL Import Wizard - Error\n",
            "\n",
            "=====Body=====\n",
            "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
            "\n",
            "<p>In Excel, the column giving me trouble looks like this:\n",
            "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>Tasks > import data > Flat Source File > select file</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
            "\n",
            "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
            " (SQL Server Import and Export Wizard)</p>\n",
            "\n",
            "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
            "\n",
            "===Keywords===\n",
            "column 0.363\n",
            "data 0.299\n",
            "import 0.29\n",
            "wizard 0.246\n",
            "sql 0.229\n",
            "decimal 0.205\n",
            "conversion 0.205\n",
            "status 0.159\n",
            "file 0.157\n",
            "server 0.123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwH6bWHrLHvC"
      },
      "source": [
        "# New section"
      ]
    }
  ]
}